{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 23:29:22,209 - INFO - Начинаю обработку URL: https://www.afisha.ru/spb/cinema/\n",
      "2025-01-05 23:29:22,210 - INFO - Обрабатываю URL: https://www.afisha.ru/spb/cinema/\n",
      "2025-01-05 23:30:24,297 - INFO - Закончил обработку URL: https://www.afisha.ru/spb/cinema/\n",
      "_____________________________________________________________________________________________\n",
      "2025-01-05 23:30:24,301 - INFO - Начинаю обработку URL: https://www.afisha.ru/spb/theatre/\n",
      "2025-01-05 23:30:24,305 - INFO - Обрабатываю URL: https://www.afisha.ru/spb/theatre/\n",
      "2025-01-05 23:31:31,548 - INFO - Закончил обработку URL: https://www.afisha.ru/spb/theatre/\n",
      "_____________________________________________________________________________________________\n",
      "2025-01-05 23:31:31,551 - INFO - Начинаю обработку URL: https://www.afisha.ru/spb/concerts/\n",
      "2025-01-05 23:31:31,552 - INFO - Обрабатываю URL: https://www.afisha.ru/spb/concerts/\n",
      "2025-01-05 23:32:22,108 - INFO - Закончил обработку URL: https://www.afisha.ru/spb/concerts/\n",
      "_____________________________________________________________________________________________\n",
      "2025-01-05 23:32:22,110 - INFO - Начинаю обработку URL: https://www.afisha.ru/spb/exhibitions/\n",
      "2025-01-05 23:32:22,112 - INFO - Обрабатываю URL: https://www.afisha.ru/spb/exhibitions/\n",
      "2025-01-05 23:32:57,342 - INFO - Закончил обработку URL: https://www.afisha.ru/spb/exhibitions/\n",
      "_____________________________________________________________________________________________\n",
      "2025-01-05 23:32:57,344 - INFO - Начинаю обработку URL: https://www.afisha.ru/msk/cinema/\n",
      "2025-01-05 23:32:57,345 - INFO - Обрабатываю URL: https://www.afisha.ru/msk/cinema/\n",
      "2025-01-05 23:33:50,264 - INFO - Закончил обработку URL: https://www.afisha.ru/msk/cinema/\n",
      "_____________________________________________________________________________________________\n",
      "2025-01-05 23:33:50,268 - INFO - Начинаю обработку URL: https://www.afisha.ru/msk/theatre/\n",
      "2025-01-05 23:33:50,268 - INFO - Обрабатываю URL: https://www.afisha.ru/msk/theatre/\n",
      "2025-01-05 23:34:48,764 - INFO - Закончил обработку URL: https://www.afisha.ru/msk/theatre/\n",
      "_____________________________________________________________________________________________\n",
      "2025-01-05 23:34:48,767 - INFO - Начинаю обработку URL: https://www.afisha.ru/msk/concerts/\n",
      "2025-01-05 23:34:48,768 - INFO - Обрабатываю URL: https://www.afisha.ru/msk/concerts/\n",
      "2025-01-05 23:35:31,459 - INFO - Закончил обработку URL: https://www.afisha.ru/msk/concerts/\n",
      "_____________________________________________________________________________________________\n",
      "2025-01-05 23:35:31,464 - INFO - Начинаю обработку URL: https://www.afisha.ru/msk/exhibitions/\n",
      "2025-01-05 23:35:31,466 - INFO - Обрабатываю URL: https://www.afisha.ru/msk/exhibitions/\n",
      "2025-01-05 23:36:13,548 - INFO - Закончил обработку URL: https://www.afisha.ru/msk/exhibitions/\n",
      "_____________________________________________________________________________________________\n",
      "2025-01-05 23:36:13,551 - INFO - Завершена обработка всех URL\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "import psycopg2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def fetch_data(url):\n",
    "    logging.info(f\"Обрабатываю URL: {url}\")\n",
    "    data = []\n",
    "    \n",
    "    if os.path.exists(\"Image\") and os.path.isdir(\"Image\"):\n",
    "        shutil.rmtree(\"Image\")\n",
    "        logging.info(\"Папка 'Image' со старыми данными была удалена\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        response_text = response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Ошибка при выполнении запроса для {url}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"name\", \"description\", \"image\", \"town\", \"type\"])\n",
    "    \n",
    "    pattern = re.compile(\n",
    "        r'{\"name\":\"(?P<name>.*?)\",.*?\"image\":\"(?P<image>.*?)\",.*?\"url\":\"(?P<url>.*?)\"'\n",
    "    )\n",
    "    \n",
    "    def get_description(detail_url):\n",
    "        try:\n",
    "            response = requests.get(detail_url)\n",
    "            response.raise_for_status()\n",
    "            html_content = response.text\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "            description_container = soup.find(\"div\", class_=\"aEVDY t1V2l\")\n",
    "            if description_container:\n",
    "                return description_container.get_text(strip=True)\n",
    "            else:\n",
    "                return \"\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Ошибка запроса: {e}\"\n",
    "        except Exception as e:\n",
    "            return f\"Ошибка: {e}\"\n",
    "    \n",
    "    try:\n",
    "        for match in pattern.finditer(response_text):\n",
    "            full_url = \"https://www.afisha.ru\" + match.group(\"url\")\n",
    "            data.append({\n",
    "                \"name\": match.group(\"name\"),\n",
    "                \"description\": get_description(full_url),\n",
    "                \"image\": match.group(\"image\"),\n",
    "                \"url\": full_url\n",
    "            })\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ошибка при парсинге данных для {url}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"name\", \"description\", \"image\", \"town\", \"type\"])\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame(data, columns=[\"name\", \"description\", \"image\"])\n",
    "        if df.empty:\n",
    "            logging.warning(f\"Данные не найдены или не удалось их распарсить для {url}.\")\n",
    "        else:\n",
    "            if \"spb\" in url:\n",
    "                df[\"town\"] = \"Санкт-Петербург\"\n",
    "            else:\n",
    "                df[\"town\"] = \"Москва\"\n",
    "            if \"cinema\" in url:\n",
    "                df[\"type\"] = \"cinema\"\n",
    "            elif \"theatre\" in url:\n",
    "                df[\"type\"] = \"theatre\"\n",
    "            elif \"concerts\" in url:\n",
    "                df[\"type\"] = \"concerts\"\n",
    "            elif \"exhibitions\" in url:\n",
    "                df[\"type\"] = \"exhibitions\"\n",
    "        logging.info(f\"Закончил обработку URL: {url}\\n_____________________________________________________________________________________________\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ошибка при создании DataFrame для {url}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"name\", \"description\", \"image\", \"town\", \"type\"])\n",
    "\n",
    "urls = [\n",
    "    \"https://www.afisha.ru/spb/cinema/\",\n",
    "    \"https://www.afisha.ru/spb/theatre/\",\n",
    "    \"https://www.afisha.ru/spb/concerts/\",\n",
    "    \"https://www.afisha.ru/spb/exhibitions/\",\n",
    "    \"https://www.afisha.ru/msk/cinema/\",\n",
    "    \"https://www.afisha.ru/msk/theatre/\",\n",
    "    \"https://www.afisha.ru/msk/concerts/\",\n",
    "    \"https://www.afisha.ru/msk/exhibitions/\"\n",
    "]\n",
    "\n",
    "parsed_data = pd.DataFrame()\n",
    "for url in urls:\n",
    "    logging.info(f\"Начинаю обработку URL: {url}\")\n",
    "    df = fetch_data(url)\n",
    "    parsed_data = pd.concat([parsed_data, df], ignore_index=True)\n",
    "\n",
    "logging.info(\"Завершена обработка всех URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correct_url(url):\n",
    "    match = re.search(r'(https?://\\S+\\.(?:jpg|jpeg|png|gif|webp))', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def clean_df(df):\n",
    "    def has_unwanted_chars(description):\n",
    "        if not isinstance(description, str):\n",
    "            return False\n",
    "        if re.search(r\"<[^>]*>\", description):\n",
    "            return True\n",
    "        return False\n",
    "    df_cleaned = df[~df['description'].apply(has_unwanted_chars) & df['description'].str.strip().astype(bool)]\n",
    "    return df_cleaned\n",
    "\n",
    "def replace_illegal_characters(text):\n",
    "    if isinstance(text, str):\n",
    "        return ''.join(char if char.isprintable() else ' ' for char in text)\n",
    "    return text\n",
    "\n",
    "clean_data = (clean_df(parsed_data)\n",
    "              .assign(image=lambda x: x['image'].apply(make_correct_url),\n",
    "                      description=lambda x: x['description'].apply(replace_illegal_characters))\n",
    "              .drop_duplicates(subset=['name', 'town', 'type'])\n",
    "              .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 23:41:19,751 - INFO - Обработка завершена\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"Image\"\n",
    "paths = []\n",
    "\n",
    "if not os.path.exists(image_folder):\n",
    "    os.makedirs(image_folder)\n",
    "\n",
    "def download_image(url, folder, filename):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return file_path\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Ошибка при скачивании {url}: {e}\")\n",
    "        if response is not None:\n",
    "            logging.error(f\"Ответ сервера: {response.status_code} - {response.text[:100]}\")\n",
    "        return None\n",
    "    \n",
    "for index, row in clean_data.iterrows():\n",
    "    image_url = row[\"image\"]\n",
    "    if pd.notna(image_url) and isinstance(image_url, str):\n",
    "        if image_url:\n",
    "            filename = f\"image_{index}.jpg\"\n",
    "            file_path = download_image(image_url, image_folder, filename)\n",
    "            if file_path:\n",
    "                paths.append(file_path)\n",
    "            else:\n",
    "                paths.append(None)\n",
    "        else:\n",
    "            logging.warning(f\"Некорректный URL: {image_url}\")\n",
    "            paths.append(None)\n",
    "    else:\n",
    "        paths.append(None)\n",
    "\n",
    "clean_data['path'] = paths\n",
    "clean_data.drop(columns=['image'], inplace=True)\n",
    "\n",
    "logging.info(\"Обработка завершена\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 23:45:14,172 - INFO - Подключение к базе данных успешно выполнено\n",
      "2025-01-05 23:45:14,208 - INFO - Таблица создана или уже существует\n",
      "2025-01-05 23:45:14,502 - INFO - Данные успешно записаны в базу данных\n",
      "2025-01-05 23:45:14,505 - INFO - Соединение с базой данных закрыто\n"
     ]
    }
   ],
   "source": [
    "connection_line = \"dbname='events' user='postgres' password='postgres' host='localhost' sslmode='disable'\"\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(connection_line)\n",
    "    cursor = conn.cursor()\n",
    "    logging.info(\"Подключение к базе данных успешно выполнено\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Ошибка подключения к базе данных: {e}\")\n",
    "\n",
    "query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS events (\n",
    "    name VARCHAR(500) NOT NULL,\n",
    "    description TEXT NOT NULL,\n",
    "    town VARCHAR(100) NOT NULL,\n",
    "    type VARCHAR(100) NOT NULL,\n",
    "    path VARCHAR(100) NOT NULL\n",
    ");\n",
    "TRUNCATE TABLE events;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "    conn.commit()\n",
    "    logging.info(\"Таблица создана или уже существует\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Ошибка при создании таблицы: {e}\")\n",
    "\n",
    "def insert_data(df, conn):\n",
    "    try:\n",
    "        for index, row in df.iterrows():\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO events (name, description, town, type, path)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "            \"\"\", (row['name'], row['description'], row['town'], row['type'], row['path']))\n",
    "        conn.commit()\n",
    "        logging.info(\"Данные успешно записаны в базу данных\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ошибка при записи данных: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "insert_data(clean_data, conn)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "logging.info(\"Соединение с базой данных закрыто\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
